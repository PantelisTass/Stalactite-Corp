library("dplyr") 
library(comprehenr)
# for scraping
library(rvest)
# blanket import for core tidyverse packages
library(tidyverse)
# tidy financial analysis 
library(tidyquant)
# tidy data cleaning functions
library(janitor)
library(quantmod)
library(ggplot2)
library(reshape2)

symbol = 'AAPL'
startDate = '2020-01-01'
endDate = '2020-12-31'
actual = getSymbols(QQQ = 'yahoo', 'AAPL', from = '2020-01-01', to = '2020-12-31', auto.assign = FALSE, env = FALSE)
Symbol = getSymbols(QQQ = 'yahoo', 'AAPL', to = as.Date('2020-01-01')-1, auto.assign = FALSE, env = FALSE)
PriceMat = c()

vec = as.vector(Symbol[,2])

Daily_returns = c()

for(i in 1:(length(vec)-1)){
  Daily_returns = c(Daily_returns, log(vec[i+1]/vec[i]))
}

mu = mean(Daily_returns)
sigma = sd(Daily_returns)
drift = mu-(sigma**2)/2



days = as.numeric(as.Date(endDate)-as.Date(startDate))
t = seq(0, days, 1)
for (i in 0:days){
  t[i+1] = as.Date(startDate) + i
}
dates = as.Date(t)
Intersect = as.Date(intersect(as.vector(dates), as.vector(index(actual))))
diff = setdiff(as.vector(dates), as.vector(index(actual)))

Corr = c()
get_correlation = function(){
  
  set.seed(NULL)
  returns = c(Daily_returns[length(Daily_returns)])
  for(n in 1:days){
    returns = c(returns, returns[length(returns)] + drift + sigma*rnorm(n = 1, mean = 0, sd = 1))
  }
  Vec = vec[length(vec)]*exp(returns)
  Indices = as.numeric(as.Date(Intersect)-as.Date('2020-01-01'))

  x = as.vector(cor(Vec[Indices], actual[,2]))
  Corr <<- c(Corr, x)
}


xvals = 1:1000
for (i in xvals){
  get_correlation()
}
print(cummean(Corr))


yvals = cummean(Corr)
plot(xvals, yvals, 
     xlab="N", 
     ylab="Mean Correlation", cex = 0.2)
lines(xvals, yvals, col = 'red')

moving_average = cummean(Corr)
differences = moving_average[2:length(moving_average)]-moving_average[1:length(moving_average)-1]
xvals1 = seq(1,1000,20)
plot(xvals1, differences[xvals1], cex = 0.2)
lines(xvals1, differences[xvals1], col = 'red')
